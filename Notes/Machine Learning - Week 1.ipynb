{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "\n",
    "**Machine Learning:**\n",
    " - Computers learn w/o being programmed\n",
    " - Computers learn from experience $E$ w/ respect to task $T$ if performance, as measured by $P$ improves w/ experience $E$\n",
    " \n",
    "**Supervised Learning:** Algorithm is taught (fed correct data)\n",
    " - *Regression:* Predict a continuous valued ouptut\n",
    " - *Classification:* Predict a discrete valued output\n",
    "\n",
    "**Unsupervised Learning:** Algorithm learns by itself (no correct data)\n",
    " - *Clustering:* Automatically group data into clusters\n",
    "\n",
    "Others: **reinforcement learning**, **recommender systems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression:** Fit a line through the data and use to predict\n",
    " - *Univariate/Simple:* One input variable\n",
    " - *Multivariate/Multiple:* More than one input variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation\n",
    "\n",
    "$m$ - Number of training examples, $x$ - Input variables, $y$ - Output variables\n",
    "\n",
    "$(x,y)$ - One training example, $(x_i,y_i)$ - $i$th training example\n",
    "\n",
    "Training Set $\\rightarrow$ Learning Algorithm $\\rightarrow$ $h$\n",
    "\n",
    "**Hypothesis:** $h$ maps $x$'s to $y$'s $\\rightarrow$ In univariate regrission, this is: $h_{\\theta}(x) = \\theta_0 + \\theta_1x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost\n",
    "\n",
    "**Cost Function:** A measure of how well the line fits\n",
    "\n",
    "**MSE (Mean Squared Error):** A measure of the average difference between predicted and actual values $\\rightarrow$ $J(\\theta_0,\\theta_1) = {\\frac{1}{2m}} {\\sum_{i=1}^m {(h_{\\theta}(x_i) - y_i)^2}}$\n",
    " - We square to get rid of negatives\n",
    " - We divide average by 2 to make gradient descent differentiation easier\n",
    " - MSE can be visualized as a function of $\\theta_0, \\theta_1$ using contour plots (2D depictions of 3D plots)\n",
    " - Goal of linear regression is to $\\min_{\\theta_0, \\theta_1} J(\\theta_0,\\theta_1) = \\min_{\\theta_0, \\theta_1} {\\frac{1}{2m}} {\\sum_{i=1}^m {(h_{\\theta}(x_i) - y_i)^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "**Gradient Descent Algorithm:** Used to find minimums on functions: Start with some $\\theta_0, \\theta_1$ $\\rightarrow$ Change $\\theta_0, \\theta_1$ until minimum\n",
    " - Formally: $\\text{repeat until convergence: } \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0,\\theta_1)$\n",
    "     - *Simultaneously update* $j=0$ and $j=1$\n",
    " - $\\alpha$ represents the *learning rate* (speed of gradient descent / step size)\n",
    "     - Always +ve\n",
    " - $\\frac{\\partial}{\\partial \\theta_j} J(\\theta_0,\\theta_1)$ represents the *derivative term*\n",
    "     - The slope of the line at the current point controls the direction of descent\n",
    "\n",
    "\n",
    "Notes:\n",
    " - Small values of $a$ causes slow descent\n",
    " - Large values of $a$ could overshoot, might diverge instead of converge\n",
    " - Do not need to update $a$, algorithm will take smaller steps as slope flattens closer to minimum\n",
    " - **Batch Gradient Descent:** Each step of the gradient descent uses all training examples\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent and Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key derivatives:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_j} J(\\theta_0,\\theta_1)$\n",
    "\n",
    "$ = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2m} {\\sum_{i=1}^m {(h_{\\theta}(x_i) - y_i)^2}}$\n",
    "\n",
    "$ = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2m} {\\sum_{i=1}^m {(\\theta_0 + \\theta_1x_i - y_i)^2}}$\n",
    "\n",
    "$\\text{Case 1}\\rightarrow j=0: \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0,\\theta_1) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta x_i - y_i)$\n",
    "\n",
    "$\\text{Case 2}\\rightarrow j=1: \\frac{\\partial}{\\partial \\theta_1} J(\\theta_0,\\theta_1) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta x_i - y_i) \\cdot x_i$\n",
    "\n",
    "The final gradient descent algorithm is therefore:\n",
    "\n",
    "$\\text{repeat until convergence: }$    \n",
    "&emsp; $\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta x_i - y_i) \\\\\n",
    "\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta x_i - y_i) \\cdot x_i \\\\\n",
    "\\text{return } \\theta_0, \\theta_1$\n",
    "\n",
    "\n",
    "Cost function for linear regression will always be *convex*, meaning it has only one optimum and it is a global one - don't have to choose between local optima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scrap Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{\\theta_0, \\theta_1} J(\\theta_0,\\theta_1) = \\min_{\\theta_0, \\theta_1} {\\frac{1}{2m}} {\\sum_{i=1}^m {(h_{\\theta}(x_i) - y_i)^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{repeat until convergence: } \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0,\\theta_1)\n",
    "\\\\ \\text{ for } j=0, j=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{repeat until convergence: } \\\\\n",
    "\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta x_i - y_i) \\\\\n",
    "\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta x_i - y_i) \\cdot x_i \\\\\n",
    "\\text{return } \\theta_0, \\theta_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{return } \\theta_0 + \\theta_1x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
