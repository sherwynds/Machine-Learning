{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Machine Learning - Week 3</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Classification Problems</center></h2>\n",
    "\n",
    "**Classification:** Discretely valued output ($y$) variable\n",
    "\n",
    "**Binary Classification:** $y \\in \\{0,1\\}$ where 0 = Negative Class (ex. benign tumor), 1 = Positive Class (ex. malignant tumor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Logistic Regression</center></h2>\n",
    "\n",
    "### Hypothesis Representation\n",
    "\n",
    "Logistic regression classifies values between $0 \\leq h_\\theta(x) \\leq 1$\n",
    "\n",
    "**Hypothesis:** $h_\\theta(x) = g(\\theta^Tx) = h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}$\n",
    "\n",
    "**Sigmoid/Logistic Function:** $g(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "![](img/sigmoid.png)\n",
    "\n",
    "$h_\\theta(x)$ is the estimated probability that $y = 1$ on input $x$\n",
    " - Mathetmatically, $h_\\theta(x) = P(y=1 \\space | \\space x;\\theta)$ means probability that $y=1$ given $x$, parameterized by $\\theta$\n",
    " - Predict $y = 1$ if $h_\\theta(x) \\geq 0.5$, $y = 0$ if $h_\\theta(x) < 0.5$\n",
    " - Equivalently: Predict $y = 1$ if $\\theta^Tx \\geq 0$, $y = 0$ if $\\theta^Tx < 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary\n",
    "**Decision Boundary:** The regression defined by $\\theta^Tx = 0$ - not always linear\n",
    " - Linear Decision Boundary ![](img/linear_decision_boundary.png)\n",
    " - Non-Linear Decision Boundary ![](img/non-linear_decision_boundary.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "Existing Cost Function: $J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\frac{1}{2}(h_\\theta(x^{(i)}) - y^{(i)})^2 = \\frac{1}{m} \\sum_{i=1}^{m} \\text{Cost}(h_\\theta(x^{(i)}),y^{(i)})$ where $Cost(h_\\theta(x), y) = \\frac{1}{2}(h_\\theta(x) - y)$\n",
    "- Produces a *non-convex* curve, with many local extrema ![](img/non-convex.png)\n",
    "- We want a *convex* curve ![](img/convex.png)\n",
    "\n",
    "Instead: $\\text{Cost}(h_\\theta(x),y) = \\begin{cases} -\\log(h\\theta(x)) \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\text{ if } y=1 \\\\ -\\log((1-h_\\theta(x)) \\space \\space \\text{ if } y=0 \\end{cases} \\space \\space \\space \\space \\leftrightarrow \\space \\space \\space \\space -y\\log(h_\\theta(x)) - (1-y)\\log(1-h_\\theta(x))$\n",
    "\n",
    "![](img/logistic_regression_1.png) ![](img/logistic_regression_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost Function:** $J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)}\\log (h_\\theta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))]$\n",
    "\n",
    "**Vectorized Cost Function:** $J(\\theta) = \\frac{1}{m}\\cdot(-y^T\\log(h)-(1-y)^T\\log(1-h))$ where $h = g(X\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Goal is to $\\min_\\theta J(\\theta)$ simultaneously updating each $\\theta_j$\n",
    "\n",
    "$\\text{Repeat } \\{ \\\\\n",
    "\\space \\space \\space \\space \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta) = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})\\cdot x_j^{(i)} \\\\\n",
    "\\}$\n",
    "\n",
    "**Vectorized Gradient Descent:** $\\theta := \\theta - \\frac{\\alpha}{m}X^T(g(X\\theta)-y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Optimization\n",
    "\n",
    "Other optimization algorithms:\n",
    " - *Conjugate Gradient*\n",
    " - *BFGS*\n",
    " - *L-BFGS*\n",
    "\n",
    "Advantages:\n",
    " - No need to manually pick $\\alpha$, pick automatically each iteration\n",
    " - Often faster than gradient descent\n",
    "\n",
    "Disadvantages:\n",
    " - More complex\n",
    " \n",
    "*Find libraries to implement these in Python (`fminunc`)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification\n",
    "\n",
    "**One vs. All:** Take multiple classes and convert each to a binary classification subproblem\n",
    "- Train logistic classifier $h_\\theta^{(i)}(x)$ for each class $i$ to predict probability that $y=i$\n",
    "- On each new input, pick the class $i$ that maximizes: $\\max_ih_\\theta^{(i)}(x)$\n",
    "![](img/one_vs_all.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Solving Overfitting</center></h2>\n",
    "\n",
    "### Problem of Overfitting\n",
    "\n",
    "**Underfitting:** Regression not fitting the data very well - *high bias* (eg. to fit a line rather than a quadratic function)\n",
    "\n",
    "**Overfitting:** Algorithm fits the training data too exactly, but fails to generalize on new exampeles - *high variance*  in hypothesis comes with too many factors\n",
    "\n",
    "We can either:\n",
    "- Reduce # of features\n",
    "     - Manually select which features to keep\n",
    "     - Model selection algorithm\n",
    "- Regularization: \n",
    "    - Keep all features but reduce magnitude of parameters $\\theta_j$\n",
    "    - Works well with lots of features, each of which is useful for predicting $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Small values for parameters $\\theta_0, \\theta_1, \\ldots, \\theta_n$\n",
    " - Simpler hypothesis (smoother)\n",
    " - Less prone to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Cost Function\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m}[\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2 + \\lambda \\sum_{j=1}^n \\theta_j^2]$\n",
    "- *Regularization parameter* $\\lambda$: Controls the tradeoff between fitting the training data (cost term) and simplifying the hypothesis (regularization term)\n",
    "- *Regularization term* $\\lambda \\sum_{j=1}^n \\theta_j^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "Originally:\n",
    "\n",
    "$\\text{Repeat } \\{ \\\\\n",
    "\\space \\space \\space \\space \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)} \\\\\n",
    "\\space \\space \\space \\space \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \\\\\n",
    "\\}$\n",
    "\n",
    "Now:\n",
    "\n",
    "$\\text{Repeat } \\{ \\\\\n",
    "\\space \\space \\space \\space \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)} \\\\\n",
    "\\space \\space \\space \\space \\theta_j := \\theta_j - \\alpha [(\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}) + \\frac{\\lambda}{m}\\theta_j] \\\\\n",
    "\\}$\n",
    "\n",
    "Note the term in square brackets is $\\frac{\\partial}{\\partial \\theta_j}J(\\theta)$ of the regularized $J(\\theta)$\n",
    "\n",
    "Equivalently:\n",
    "\n",
    "$\\text{Repeat } \\{ \\\\\n",
    "\\space \\space \\space \\space \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)} \\\\\n",
    "\\space \\space \\space \\space \\theta_j := \\theta_j(1-\\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}\\\\\n",
    "\\}$\n",
    "\n",
    "Note: $1-\\alpha \\frac{\\lambda}{m} < 1$ usually slightly less than 1 for $\\alpha, \\lambda, m > 0$, therefore, multiplying by $\\theta_j$ shrink the parameter's magnitude\n",
    "\n",
    "#### Normal Equation\n",
    "\n",
    "Normal equation now becomes ($\\lambda > 0$):\n",
    "\n",
    "$\\theta = (X^TX + \\lambda L)^{-1}X^Ty$\n",
    "\n",
    "where $L = \\begin{bmatrix}0\\\\&1\\\\&&1\\\\&&&\\ddots\\\\&&&&1\\end{bmatrix}$\n",
    "\n",
    "The new term is $\\lambda$ multiplied by an $(n+1) \\times (n+1)$ matrix\n",
    "\n",
    "Note: Regularization removes issues related to non-invertibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression\n",
    "\n",
    "**Regularized Cost Function:** $J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)}\\log (h_\\theta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^n\\theta_j^2$\n",
    "\n",
    "**Regularized Gradient Descent:**\n",
    "\n",
    "$\\text{Repeat } \\{ \\\\\n",
    "\\space \\space \\space \\space \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)} \\\\\n",
    "\\space \\space \\space \\space \\theta_j := \\theta_j - \\alpha [(\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}) + \\frac{\\lambda}{m}\\theta_j] \\\\\n",
    "\\}$\n",
    "\n",
    "Note: looks the same as linear regression, but is different because hypothesis is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
