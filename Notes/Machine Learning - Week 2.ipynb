{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Notation\n",
    "\n",
    "**Multiple Linear Regression:** Use more than one feature to estimate the dependent variable\n",
    " - $n$ = number of features, $m$ = number of training examples\n",
    " - $x^{(i)}$ = features of the $i^{th}$ training example (vector)\n",
    " - $x_j$ = input feature $j$\n",
    " - $x_j^{(i)}$ = value of feature $j$ in the $i^{th}$ training example\n",
    " \n",
    " \n",
    " **Hypothesis Function:** $h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\ldots + \\theta_nx_n \\space \\space \\leftrightarrow \\space \\space h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\ldots + \\theta_nx_n$\n",
    "  - Define $x_0 = 1 \\space \\leftrightarrow \\space x_0^{(i)} = 1$ for all training examples $i$\n",
    "  - $n+1$ dimensional feature vector $x$ and $n+1$ dimensional parameter vector $\\theta$ (0-indexed)\n",
    "  \n",
    "  $x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\in \\mathbb{R}^{n+1} \\space \\space \\space \\space \\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} \\in \\mathbb{R}^{n+1}$\n",
    "  - Can be written as: $\\theta^Tx = \\begin{bmatrix} \\theta_0 \\space \\space \\space \\theta_1 \\space \\space \\space \\dots \\space \\space \\space \\theta_n \\end{bmatrix} \\cdot \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost Function:** $J(\\theta_0,\\theta_1,\\ldots,\\theta_n) = J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y{(i)})^2$\n",
    "\n",
    "**Gradient Descent Algorithm:**\n",
    "\n",
    "$\\text{repeat } \\{$\n",
    "\n",
    "&emsp;$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta) \\space \\space \\leftrightarrow \\space \\space \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}$\n",
    "\n",
    "$\\} \\space (\\text{simultaneously update for every } j = 0,\\ldots,n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "**Feature Scaling:** Make sure features are on a similar scale\n",
    " - Motivation: if variables are of vastly different magnitude, the contour graph will be quite skewed, therefore gradient descent will take long to reach a minimum\n",
    " - Scale features into aprrox. a $-1 \\leq x_i \\leq 1$ range $\\rightarrow$ does not need to be the same for each feature, but should be within:\n",
    "     - Max of $\\pm 3$ \n",
    "     - Min of $\\pm \\frac{1}{3}$\n",
    "\n",
    "### Types of Feature Scaling\n",
    "\n",
    "**Standard Feature Scaling:** $x_i := \\frac{x_i}{s_i)}$ where $s_i$ is the range (max - min)\n",
    "\n",
    "**Mean Normalization:** $x_i := \\frac{x_i-\\mu_i}{s_i}$ where $s_i$ is the range (max - min) or standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
