{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Notation\n",
    "\n",
    "**Multiple Linear Regression:** Use more than one feature to estimate the dependent variable\n",
    " - $n$ = number of features, $m$ = number of training examples\n",
    " - $x^{(i)}$ = features of the $i^{th}$ training example (vector)\n",
    " - $x_j$ = input feature $j$\n",
    " - $x_j^{(i)}$ = value of feature $j$ in the $i^{th}$ training example\n",
    " \n",
    " \n",
    " **Hypothesis Function:** $h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\ldots + \\theta_nx_n \\space \\space \\leftrightarrow \\space \\space h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\ldots + \\theta_nx_n$\n",
    "  - Define $x_0 = 1 \\space \\leftrightarrow \\space x_0^{(i)} = 1$ for all training examples $i$\n",
    "  - $n+1$ dimensional feature vector $x$ and $n+1$ dimensional parameter vector $\\theta$ (0-indexed)\n",
    "  \n",
    "  $x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\in \\mathbb{R}^{n+1} \\space \\space \\space \\space \\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} \\in \\mathbb{R}^{n+1}$\n",
    "  - Can be written as: $\\theta^Tx = \\begin{bmatrix} \\theta_0 \\space \\space \\space \\theta_1 \\space \\space \\space \\dots \\space \\space \\space \\theta_n \\end{bmatrix} \\cdot \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost Function:** $J(\\theta_0,\\theta_1,\\ldots,\\theta_n) = J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y{(i)})^2$\n",
    "\n",
    "**Gradient Descent Algorithm:**\n",
    "\n",
    "$\\text{repeat } \\{$\n",
    "\n",
    "&emsp;$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta) \\space \\space \\leftrightarrow \\space \\space \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}$\n",
    "\n",
    "$\\} \\space (\\text{simultaneously update for every } j = 0,\\ldots,n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Practice\n",
    "\n",
    "### Feature Scaling\n",
    "\n",
    "**Feature Scaling:** Make sure features are on a similar scale $\\rightarrow$ $x_i := \\frac{x_i}{s_i}$ where $s_i$ is the range (max - min)\n",
    " - If variables are of vastly different magnitude, the contour graph will be quite skewed, therefore gradient descent will take long to reach a minimum\n",
    " - Scale features into aprrox. a $-1 \\leq x_i \\leq 1$ range $\\rightarrow$ does not need to be the same for each feature, but should be within:\n",
    "     - Max of $\\pm 3$ \n",
    "     - Min of $\\pm \\frac{1}{3}$\n",
    "\n",
    "\n",
    "**Mean Normalization:** $x_i := \\frac{x_i-\\mu_i}{s_i}$ where $s_i$ is the range (max - min) or standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing $\\alpha$ Value\n",
    "\n",
    "![](img/check-gd-working.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot cost as a function of the number of iterations, and check if decreasing with each iteration $\\rightarrow$ can check convergence visually\n",
    " - Algorithm to check convergence: Declare convergence if $J(\\theta)$ decreases by less than some small value $\\epsilon$ in one iteration (for example $10^{-3}$)\n",
    " - If $J(\\theta)$ increases with iterations, or alternates between increasing and decreasing, gradient descent isn't working $\\rightarrow$ use a smaller $\\alpha$\n",
    " - To choose $\\alpha$, try $\\ldots, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, \\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features can be calculated; for example, given $x_1 = \\text{length}$ and $x_2 = \\text{width}$ we might simply use $x = \\text{area}$ to predict instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Regression:** Fit a polynomial curve instead of a straight line to the data: $h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 \\leftrightarrow \\theta_0 + \\theta_1x_1 + \\theta_2x_2$ where $x_1 = x$ and $x_2 = x^2$\n",
    "- Exponents can be fractional (square roots) as well\n",
    "- Feature scaling may be important when dealing with higher order polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
